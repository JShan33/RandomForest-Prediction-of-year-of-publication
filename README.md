# RandomForest-Prediction-of-year-of-publication
Prediction of year of publication of science paper based on RandomForest algorithm
## Feature engineering
Our code applied a range of feature engineering techniques to the original 6 features. We first
combined editor and author into editor_author and extracted new features about the number of authors and the number of editors. From “edit_author” the textual features “title” and “publisher”, we extracted the word unigrams in the names, titles and abstracts, subsequently vectorising the first using count vectorizer and the rest with TF-IDF vectorizer, because for titles and abstracts the most frequent words often provide least information, which TF-IDF considers. For entrytype and publisher, we used one hot encoder for feature transformation as they are categorical.
## Learning algorithm(s)
We have tried different learning algorithms,including Ridge/Lasso regression, RidgeCV , LassoCV ,RandomForestRegressor,Deep Neural Network, K-fold cross validation.etc.After many trials, we finally chose the learning algorithm based on RandomForestRegressor and on this basis, we applied feature selection and model selection. First ,we fit all the training data using the RandomForestRegressor model. Secondly,We implemented feature selection through SelectFromModel in ‘sklearn.model_selection’ library to get the new feature (X_new) that was well filtered (the shape of train data reduced from (65914,140589) to (65914,4893)). Thirdly , we implemented model selection through cross_val_score in ‘sklearn.model_selection’ library，to fit a new RandoForestRegressor with X_new and find the best model through cross validation.
## Hyperparameter tuning
After the initial selection of the RandomForest model, we encountered computational challenges attributable to the high dimensionality of the transformed data. In response, we adjusted the hyperparameter ngram_range of the TfidfVectorizer from (1, 3) to (1, 1), effectively reducing the dimensionality of the data from 4,464,048 to 140,591.This modification facilitated more efficient model execution without compromising the essential information encoded in the data. Moreover, to enhance the predictive capabilities of the model, we tuned the hyperparameter n_estimators of the RandoForestRegressor from 50 to 100.
## Discussion of the performance of your solution
The performance of our solution can be discussed in terms of the Mean Absolute Error (MAE) on both the training and testing sets, as well as the results from 5-fold cross-validation. The training MAE is 1.16, and the testing MAE is 3.22. We can see that there is overfitting of the model, but the testing MAE is still relatively low, suggesting that our model generalizes well and is able to make accurate predictions on new data. The final test MAE in Codalab is 3.08, which is consistent with the testing MAE and significantly lower than the baseline model’s MSE of 5.61. This indicates that our choice of algorithm and hyperparameter tuning is effective.
